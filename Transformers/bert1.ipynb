{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-08T18:27:54.213338Z","iopub.status.busy":"2024-03-08T18:27:54.212955Z","iopub.status.idle":"2024-03-08T18:27:55.432413Z","shell.execute_reply":"2024-03-08T18:27:55.431357Z","shell.execute_reply.started":"2024-03-08T18:27:54.213295Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/hos-original-bert/training_data_telugu-hate.csv\n","/kaggle/input/transliteration/transliteration_4000.csv\n","/kaggle/input/translated/Hos_train_translated.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T18:27:55.435179Z","iopub.status.busy":"2024-03-08T18:27:55.434623Z","iopub.status.idle":"2024-03-08T18:28:10.163761Z","shell.execute_reply":"2024-03-08T18:28:10.162532Z","shell.execute_reply.started":"2024-03-08T18:27:55.435143Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["  pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T18:28:10.165542Z","iopub.status.busy":"2024-03-08T18:28:10.165221Z","iopub.status.idle":"2024-03-08T18:56:12.318034Z","shell.execute_reply":"2024-03-08T18:56:12.316979Z","shell.execute_reply.started":"2024-03-08T18:28:10.165513Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"355cfbb056e548f9b1b14af5ea8d8750","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3621f085c5144709a0fce3120a7e6ce","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa76059e0b784bc6936d2b469dbfbe47","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcca04a84696426d8a134e7b4c9a3075","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd2eadfac51c4e7c8f005f8ae325d69b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7225\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.83      0.76       430\n","           1       0.75      0.59      0.66       370\n","\n","    accuracy                           0.72       800\n","   macro avg       0.73      0.71      0.71       800\n","weighted avg       0.73      0.72      0.72       800\n","\n"]}],"source":["import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load your CSV file\n","df = pd.read_csv('/kaggle/input/hos-original-bert/training_data_telugu-hate.csv')\n","\n","# Mapping labels to integers\n","label_map = {'hate': 1, 'non-hate': 0}\n","df['Label'] = df['Label'].map(label_map)\n","\n","# Extract input texts and labels\n","texts = df['Comments'].tolist()\n","labels = df['Label'].tolist()\n","\n","# Split data into train and test sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Tokenize inputs\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n","\n","# Convert labels to PyTorch tensors\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create PyTorch datasets\n","train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n","                                               torch.tensor(train_encodings['attention_mask']),\n","                                               train_labels)\n","test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n","                                              torch.tensor(test_encodings['attention_mask']),\n","                                              test_labels)\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Set device (GPU if available, otherwise CPU)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","# Set optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","num_epochs = 20\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","model.eval()\n","predictions = []\n","true_labels = []\n","for batch in test_loader:\n","    input_ids, attention_mask, labels = batch\n","    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","    \n","    logits = outputs.logits\n","    preds = torch.argmax(logits, dim=1)\n","    \n","    predictions.extend(preds.cpu().numpy())\n","    true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate accuracy and other metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","report = classification_report(true_labels, predictions)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(report)\n"]},{"cell_type":"markdown","metadata":{},"source":["translated"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T18:56:12.320866Z","iopub.status.busy":"2024-03-08T18:56:12.320548Z","iopub.status.idle":"2024-03-08T19:37:20.253465Z","shell.execute_reply":"2024-03-08T19:37:20.251960Z","shell.execute_reply.started":"2024-03-08T18:56:12.320840Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.76375\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.83      0.79       430\n","           1       0.78      0.68      0.73       370\n","\n","    accuracy                           0.76       800\n","   macro avg       0.77      0.76      0.76       800\n","weighted avg       0.77      0.76      0.76       800\n","\n"]}],"source":["import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load your CSV file\n","df = pd.read_csv('/kaggle/input/translated/Hos_train_translated.csv')\n","\n","# Mapping labels to integers\n","label_map = {'hate': 1, 'non-hate': 0}\n","df['Label'] = df['Label'].map(label_map)\n","\n","# Extract input texts and labels\n","texts = df['comments '].tolist()\n","labels = df['Label'].tolist()\n","\n","# Split data into train and test sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Tokenize inputs\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n","\n","# Convert labels to PyTorch tensors\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create PyTorch datasets\n","train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n","                                               torch.tensor(train_encodings['attention_mask']),\n","                                               train_labels)\n","test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n","                                              torch.tensor(test_encodings['attention_mask']),\n","                                              test_labels)\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Set device (GPU if available, otherwise CPU)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","# Set optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","num_epochs = 20\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","model.eval()\n","predictions = []\n","true_labels = []\n","for batch in test_loader:\n","    input_ids, attention_mask, labels = batch\n","    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","    \n","    logits = outputs.logits\n","    preds = torch.argmax(logits, dim=1)\n","    \n","    predictions.extend(preds.cpu().numpy())\n","    true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate accuracy and other metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","report = classification_report(true_labels, predictions)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(report)\n"]},{"cell_type":"markdown","metadata":{},"source":["transliteration"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T19:37:20.256610Z","iopub.status.busy":"2024-03-08T19:37:20.256051Z","iopub.status.idle":"2024-03-08T20:09:38.399574Z","shell.execute_reply":"2024-03-08T20:09:38.398425Z","shell.execute_reply.started":"2024-03-08T19:37:20.256564Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7475\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.84      0.78       430\n","           1       0.77      0.64      0.70       370\n","\n","    accuracy                           0.75       800\n","   macro avg       0.75      0.74      0.74       800\n","weighted avg       0.75      0.75      0.74       800\n","\n"]}],"source":["import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load your CSV file\n","df = pd.read_csv('/kaggle/input/transliteration/transliteration_4000.csv')\n","\n","# Mapping labels to integers\n","label_map = {'hate': 1, 'non-hate': 0}\n","df['Label'] = df['Label'].map(label_map)\n","\n","# Extract input texts and labels\n","texts = df['Comments'].tolist()\n","labels = df['Label'].tolist()\n","\n","# Split data into train and test sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Tokenize inputs\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n","\n","# Convert labels to PyTorch tensors\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create PyTorch datasets\n","train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n","                                               torch.tensor(train_encodings['attention_mask']),\n","                                               train_labels)\n","test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n","                                              torch.tensor(test_encodings['attention_mask']),\n","                                              test_labels)\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Set device (GPU if available, otherwise CPU)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","# Set optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","num_epochs = 20\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","model.eval()\n","predictions = []\n","true_labels = []\n","for batch in test_loader:\n","    input_ids, attention_mask, labels = batch\n","    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","    \n","    logits = outputs.logits\n","    preds = torch.argmax(logits, dim=1)\n","    \n","    predictions.extend(preds.cpu().numpy())\n","    true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate accuracy and other metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","report = classification_report(true_labels, predictions)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(report)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T20:09:38.401286Z","iopub.status.busy":"2024-03-08T20:09:38.400961Z","iopub.status.idle":"2024-03-08T20:09:38.408678Z","shell.execute_reply":"2024-03-08T20:09:38.407769Z","shell.execute_reply.started":"2024-03-08T20:09:38.401261Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class Weights: {0: 0.9809932556713673, 1: 1.0197578075207139}\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","# Compute class weights\n","class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_labels.numpy())\n","\n","# Convert class weights to a dictionary\n","class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","print(\"Class Weights:\", class_weights_dict)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4562837,"sourceId":7794137,"sourceType":"datasetVersion"},{"datasetId":4562918,"sourceId":7794250,"sourceType":"datasetVersion"},{"datasetId":4562992,"sourceId":7794347,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
