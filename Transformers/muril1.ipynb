{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208cdb40-5814-4a21-9cdf-a845133ab3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "  pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d53736-136b-40e1-887a-5d5627a766bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac721582-ae61-416c-90e8-9f56479064fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: transformers in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.38.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: fsspec in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas torch scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85ea078-7395-4441-9087-8213d1d76787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78375\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79       430\n",
      "           1       0.74      0.82      0.78       370\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.78      0.79      0.78       800\n",
      "weighted avg       0.79      0.78      0.78       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/training_data_telugu-hate.csv')\n",
    "\n",
    "# Mapping labels to integers\n",
    "label_map = {'hate': 1, 'non-hate': 0}\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Extract input texts and labels\n",
    "texts = df['Comments'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained MuRIL model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n",
    "\n",
    "# Tokenize inputs\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "report = classification_report(true_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656e583-a626-4fca-a1d4-75294b53466f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c56e6a1-3627-4f36-a385-c5e1801e4cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75       430\n",
      "           1       0.72      0.69      0.70       370\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.73      0.73      0.73       800\n",
      "weighted avg       0.73      0.73      0.73       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/Hos_train_translated.csv')\n",
    "\n",
    "# Mapping labels to integers\n",
    "label_map = {'hate': 1, 'non-hate': 0}\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Extract input texts and labels\n",
    "texts = df['comments '].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained MuRIL model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n",
    "\n",
    "# Tokenize inputs\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "report = classification_report(true_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6ee00-3c95-4f5b-afb3-eef59673b0cf",
   "metadata": {},
   "source": [
    "### transliterated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abb86fe-3aa3-4e70-9886-9c4d8131f7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73875\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       430\n",
      "           1       0.71      0.74      0.72       370\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.74      0.74      0.74       800\n",
      "weighted avg       0.74      0.74      0.74       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/transliteration_4000.csv')\n",
    "\n",
    "# Mapping labels to integers\n",
    "label_map = {'hate': 1, 'non-hate': 0}\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Extract input texts and labels\n",
    "texts = df['Comments'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained MuRIL model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n",
    "\n",
    "# Tokenize inputs\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "report = classification_report(true_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47767843-60d3-4124-90b7-af28be95e971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
