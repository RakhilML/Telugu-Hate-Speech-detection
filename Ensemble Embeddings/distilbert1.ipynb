{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7794137,"sourceType":"datasetVersion","datasetId":4562837},{"sourceId":7794250,"sourceType":"datasetVersion","datasetId":4562918},{"sourceId":7794347,"sourceType":"datasetVersion","datasetId":4562992},{"sourceId":7852380,"sourceType":"datasetVersion","datasetId":4605210}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### distilbert+labse","metadata":{}},{"cell_type":"markdown","source":"original\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/training_data_telugu-hate.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['Comments'].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and LaBSE models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nlabse_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\nlabse_model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/LaBSE\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_labse = labse_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_labse = labse_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(train_encodings_labse['input_ids']),\n                                                     torch.tensor(train_encodings_labse['attention_mask']),\n                                                     train_labels)\ntest_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(test_encodings_labse['input_ids']),\n                                                    torch.tensor(test_encodings_labse['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_labse = torch.utils.data.DataLoader(train_dataset_labse, batch_size=8, shuffle=True)\ntest_loader_labse = torch.utils.data.DataLoader(test_dataset_labse, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nlabse_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_labse = AdamW(labse_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for LaBSE\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (LaBSE)\")\n    labse_model.train()\n    for batch in train_loader_labse:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_labse.zero_grad()\n        outputs = labse_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_labse.step()\n\n# Evaluation\ndistilbert_model.eval()\nlabse_model.eval()\npredictions_distilbert = []\npredictions_labse = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_labse:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = labse_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_labse.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_labse in zip(predictions_distilbert, predictions_labse):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_labse) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T16:00:28.073428Z","iopub.execute_input":"2024-03-15T16:00:28.073806Z","iopub.status.idle":"2024-03-15T16:28:19.165390Z","shell.execute_reply.started":"2024-03-15T16:00:28.073775Z","shell.execute_reply":"2024-03-15T16:28:19.164498Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (LaBSE)\nEpoch 2/10 (LaBSE)\nEpoch 3/10 (LaBSE)\nEpoch 4/10 (LaBSE)\nEpoch 5/10 (LaBSE)\nEpoch 6/10 (LaBSE)\nEpoch 7/10 (LaBSE)\nEpoch 8/10 (LaBSE)\nEpoch 9/10 (LaBSE)\nEpoch 10/10 (LaBSE)\nAccuracy: 0.76875\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.73      0.77       430\n           1       0.72      0.81      0.76       370\n\n    accuracy                           0.77       800\n   macro avg       0.77      0.77      0.77       800\nweighted avg       0.77      0.77      0.77       800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"translated\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/Hos_train_translated.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['comments '].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and LaBSE models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nlabse_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\nlabse_model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/LaBSE\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_labse = labse_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_labse = labse_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(train_encodings_labse['input_ids']),\n                                                     torch.tensor(train_encodings_labse['attention_mask']),\n                                                     train_labels)\ntest_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(test_encodings_labse['input_ids']),\n                                                    torch.tensor(test_encodings_labse['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_labse = torch.utils.data.DataLoader(train_dataset_labse, batch_size=8, shuffle=True)\ntest_loader_labse = torch.utils.data.DataLoader(test_dataset_labse, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nlabse_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_labse = AdamW(labse_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for LaBSE\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (LaBSE)\")\n    labse_model.train()\n    for batch in train_loader_labse:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_labse.zero_grad()\n        outputs = labse_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_labse.step()\n\n# Evaluation\ndistilbert_model.eval()\nlabse_model.eval()\npredictions_distilbert = []\npredictions_labse = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_labse:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = labse_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_labse.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_labse in zip(predictions_distilbert, predictions_labse):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_labse) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T16:28:19.167106Z","iopub.execute_input":"2024-03-15T16:28:19.167563Z","iopub.status.idle":"2024-03-15T17:00:31.879037Z","shell.execute_reply.started":"2024-03-15T16:28:19.167536Z","shell.execute_reply":"2024-03-15T17:00:31.878009Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (LaBSE)\nEpoch 2/10 (LaBSE)\nEpoch 3/10 (LaBSE)\nEpoch 4/10 (LaBSE)\nEpoch 5/10 (LaBSE)\nEpoch 6/10 (LaBSE)\nEpoch 7/10 (LaBSE)\nEpoch 8/10 (LaBSE)\nEpoch 9/10 (LaBSE)\nEpoch 10/10 (LaBSE)\nAccuracy: 0.76375\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.72      0.77       430\n           1       0.71      0.82      0.76       370\n\n    accuracy                           0.76       800\n   macro avg       0.77      0.77      0.76       800\nweighted avg       0.77      0.76      0.76       800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"transliterated","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/transliteration_4000.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['Comments'].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and LaBSE models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nlabse_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\nlabse_model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/LaBSE\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_labse = labse_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_labse = labse_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(train_encodings_labse['input_ids']),\n                                                     torch.tensor(train_encodings_labse['attention_mask']),\n                                                     train_labels)\ntest_dataset_labse = torch.utils.data.TensorDataset(torch.tensor(test_encodings_labse['input_ids']),\n                                                    torch.tensor(test_encodings_labse['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_labse = torch.utils.data.DataLoader(train_dataset_labse, batch_size=8, shuffle=True)\ntest_loader_labse = torch.utils.data.DataLoader(test_dataset_labse, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nlabse_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_labse = AdamW(labse_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for LaBSE\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (LaBSE)\")\n    labse_model.train()\n    for batch in train_loader_labse:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_labse.zero_grad()\n        outputs = labse_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_labse.step()\n\n# Evaluation\ndistilbert_model.eval()\nlabse_model.eval()\npredictions_distilbert = []\npredictions_labse = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_labse:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = labse_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_labse.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_labse in zip(predictions_distilbert, predictions_labse):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_labse) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T17:00:31.880859Z","iopub.execute_input":"2024-03-15T17:00:31.881175Z","iopub.status.idle":"2024-03-15T17:34:35.473234Z","shell.execute_reply.started":"2024-03-15T17:00:31.881148Z","shell.execute_reply":"2024-03-15T17:34:35.471970Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (LaBSE)\nEpoch 2/10 (LaBSE)\nEpoch 3/10 (LaBSE)\nEpoch 4/10 (LaBSE)\nEpoch 5/10 (LaBSE)\nEpoch 6/10 (LaBSE)\nEpoch 7/10 (LaBSE)\nEpoch 8/10 (LaBSE)\nEpoch 9/10 (LaBSE)\nEpoch 10/10 (LaBSE)\nAccuracy: 0.75625\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.83      0.69      0.75       430\n           1       0.70      0.83      0.76       370\n\n    accuracy                           0.76       800\n   macro avg       0.76      0.76      0.76       800\nweighted avg       0.77      0.76      0.76       800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### distilbert+muril","metadata":{}},{"cell_type":"markdown","source":"original","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/training_data_telugu-hate.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['Comments'].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and MuRIL models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nmuril_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmuril_model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_muril = muril_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_muril = muril_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(train_encodings_muril['input_ids']),\n                                                     torch.tensor(train_encodings_muril['attention_mask']),\n                                                     train_labels)\ntest_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(test_encodings_muril['input_ids']),\n                                                    torch.tensor(test_encodings_muril['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_muril = torch.utils.data.DataLoader(train_dataset_muril, batch_size=8, shuffle=True)\ntest_loader_muril = torch.utils.data.DataLoader(test_dataset_muril, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nmuril_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_muril = AdamW(muril_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for MuRIL\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (MuRIL)\")\n    muril_model.train()\n    for batch in train_loader_muril:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_muril.zero_grad()\n        outputs = muril_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_muril.step()\n\n# Evaluation\ndistilbert_model.eval()\nmuril_model.eval()\npredictions_distilbert = []\npredictions_muril = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_muril:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = muril_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_muril.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_muril in zip(predictions_distilbert, predictions_muril):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_muril) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T17:34:35.476089Z","iopub.execute_input":"2024-03-15T17:34:35.476622Z","iopub.status.idle":"2024-03-15T17:57:18.591837Z","shell.execute_reply.started":"2024-03-15T17:34:35.476586Z","shell.execute_reply":"2024-03-15T17:57:18.590651Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b580baa8b24426a8253fd617993d65d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16eebd9ff5fc42c28c5ea6166dda543e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701f47490b1545b9af763c3ec3e10c88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa3943a6a0b46e38d31a9455ee0e5eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dbab5d4eeb44f4a9222d937446214c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (MuRIL)\nEpoch 2/10 (MuRIL)\nEpoch 3/10 (MuRIL)\nEpoch 4/10 (MuRIL)\nEpoch 5/10 (MuRIL)\nEpoch 6/10 (MuRIL)\nEpoch 7/10 (MuRIL)\nEpoch 8/10 (MuRIL)\nEpoch 9/10 (MuRIL)\nEpoch 10/10 (MuRIL)\nAccuracy: 0.7725\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.80      0.77      0.78       430\n           1       0.74      0.77      0.76       370\n\n    accuracy                           0.77       800\n   macro avg       0.77      0.77      0.77       800\nweighted avg       0.77      0.77      0.77       800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"translated","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/Hos_train_translated.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['comments '].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and MuRIL models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nmuril_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmuril_model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_muril = muril_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_muril = muril_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(train_encodings_muril['input_ids']),\n                                                     torch.tensor(train_encodings_muril['attention_mask']),\n                                                     train_labels)\ntest_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(test_encodings_muril['input_ids']),\n                                                    torch.tensor(test_encodings_muril['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_muril = torch.utils.data.DataLoader(train_dataset_muril, batch_size=8, shuffle=True)\ntest_loader_muril = torch.utils.data.DataLoader(test_dataset_muril, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nmuril_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_muril = AdamW(muril_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for MuRIL\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (MuRIL)\")\n    muril_model.train()\n    for batch in train_loader_muril:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_muril.zero_grad()\n        outputs = muril_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_muril.step()\n\n# Evaluation\ndistilbert_model.eval()\nmuril_model.eval()\npredictions_distilbert = []\npredictions_muril = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_muril:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = muril_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_muril.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_muril in zip(predictions_distilbert, predictions_muril):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_muril) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T17:57:18.593269Z","iopub.execute_input":"2024-03-15T17:57:18.593585Z","iopub.status.idle":"2024-03-15T18:25:47.760455Z","shell.execute_reply.started":"2024-03-15T17:57:18.593558Z","shell.execute_reply":"2024-03-15T18:25:47.759525Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (MuRIL)\nEpoch 2/10 (MuRIL)\nEpoch 3/10 (MuRIL)\nEpoch 4/10 (MuRIL)\nEpoch 5/10 (MuRIL)\nEpoch 6/10 (MuRIL)\nEpoch 7/10 (MuRIL)\nEpoch 8/10 (MuRIL)\nEpoch 9/10 (MuRIL)\nEpoch 10/10 (MuRIL)\nAccuracy: 0.74\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.66      0.73       430\n           1       0.68      0.84      0.75       370\n\n    accuracy                           0.74       800\n   macro avg       0.75      0.75      0.74       800\nweighted avg       0.76      0.74      0.74       800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"transliterated","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/telugudat/transliteration_4000.csv')\n\n# Mapping labels to integers\nlabel_map = {'hate': 1, 'non-hate': 0}\ndf['Label'] = df['Label'].map(label_map)\n\n# Extract input texts and labels\ntexts = df['Comments'].tolist()\nlabels = df['Label'].tolist()\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained DistilBERT and MuRIL models and tokenizers\ndistilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\nmuril_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmuril_model = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n\n# Tokenize inputs\ntrain_encodings_distilbert = distilbert_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_distilbert = distilbert_tokenizer(test_texts, truncation=True, padding=True)\n\ntrain_encodings_muril = muril_tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings_muril = muril_tokenizer(test_texts, truncation=True, padding=True)\n\n# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Create PyTorch datasets\ntrain_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(train_encodings_distilbert['input_ids']),\n                                                          torch.tensor(train_encodings_distilbert['attention_mask']),\n                                                          train_labels)\ntest_dataset_distilbert = torch.utils.data.TensorDataset(torch.tensor(test_encodings_distilbert['input_ids']),\n                                                         torch.tensor(test_encodings_distilbert['attention_mask']),\n                                                         test_labels)\n\ntrain_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(train_encodings_muril['input_ids']),\n                                                     torch.tensor(train_encodings_muril['attention_mask']),\n                                                     train_labels)\ntest_dataset_muril = torch.utils.data.TensorDataset(torch.tensor(test_encodings_muril['input_ids']),\n                                                    torch.tensor(test_encodings_muril['attention_mask']),\n                                                    test_labels)\n\n# Create data loaders\ntrain_loader_distilbert = torch.utils.data.DataLoader(train_dataset_distilbert, batch_size=8, shuffle=True)\ntest_loader_distilbert = torch.utils.data.DataLoader(test_dataset_distilbert, batch_size=8, shuffle=False)\n\ntrain_loader_muril = torch.utils.data.DataLoader(train_dataset_muril, batch_size=8, shuffle=True)\ntest_loader_muril = torch.utils.data.DataLoader(test_dataset_muril, batch_size=8, shuffle=False)\n\n# Set device (GPU if available, otherwise CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndistilbert_model.to(device)\nmuril_model.to(device)\n\n# Set optimizers and learning rate schedulers\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=1e-5)\noptimizer_muril = AdamW(muril_model.parameters(), lr=1e-5)\nnum_epochs = 10\n\n# Training loop for DistilBERT\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (DistilBERT)\")\n    distilbert_model.train()\n    for batch in train_loader_distilbert:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distilbert.zero_grad()\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_distilbert.step()\n\n# Training loop for MuRIL\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} (MuRIL)\")\n    muril_model.train()\n    for batch in train_loader_muril:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_muril.zero_grad()\n        outputs = muril_model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_muril.step()\n\n# Evaluation\ndistilbert_model.eval()\nmuril_model.eval()\npredictions_distilbert = []\npredictions_muril = []\ntrue_labels = []\nfor batch in test_loader_distilbert:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_distilbert.extend(preds.cpu().numpy())\n\nfor batch in test_loader_muril:\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n    \n    with torch.no_grad():\n        outputs = muril_model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=1)\n    \n    predictions_muril.extend(preds.cpu().numpy())\n    true_labels.extend(labels.cpu().numpy())\n\n# Combine predictions from both models\nfinal_predictions = []\nfor pred_distilbert, pred_muril in zip(predictions_distilbert, predictions_muril):\n    # Simple voting scheme, you can choose a different method for combining predictions\n    combined_prediction = 1 if (pred_distilbert + pred_muril) >= 1 else 0\n    final_predictions.append(combined_prediction)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, final_predictions)\nreport = classification_report(true_labels, final_predictions)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T18:25:47.762335Z","iopub.execute_input":"2024-03-15T18:25:47.762710Z","iopub.status.idle":"2024-03-15T18:56:30.021521Z","shell.execute_reply.started":"2024-03-15T18:25:47.762676Z","shell.execute_reply":"2024-03-15T18:56:30.020460Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 (DistilBERT)\nEpoch 2/10 (DistilBERT)\nEpoch 3/10 (DistilBERT)\nEpoch 4/10 (DistilBERT)\nEpoch 5/10 (DistilBERT)\nEpoch 6/10 (DistilBERT)\nEpoch 7/10 (DistilBERT)\nEpoch 8/10 (DistilBERT)\nEpoch 9/10 (DistilBERT)\nEpoch 10/10 (DistilBERT)\nEpoch 1/10 (MuRIL)\nEpoch 2/10 (MuRIL)\nEpoch 3/10 (MuRIL)\nEpoch 4/10 (MuRIL)\nEpoch 5/10 (MuRIL)\nEpoch 6/10 (MuRIL)\nEpoch 7/10 (MuRIL)\nEpoch 8/10 (MuRIL)\nEpoch 9/10 (MuRIL)\nEpoch 10/10 (MuRIL)\nAccuracy: 0.73875\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.66      0.73       430\n           1       0.68      0.83      0.75       370\n\n    accuracy                           0.74       800\n   macro avg       0.75      0.75      0.74       800\nweighted avg       0.75      0.74      0.74       800\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}